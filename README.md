# Smart Surround
### **ğŸ… Intel Edge AI Academy í”„ë¡œì íŠ¸ ê²½ì§„ëŒ€íšŒ â€“ ìµœìš°ìˆ˜ìƒ ìˆ˜ìƒì‘**

ë„ë¡œ ìƒì—ì„œ ì£¼ë³€ ì°¨ëŸ‰ì˜ ìš´ì „ í–‰íƒœ(ê³¼ì†, ì¡¸ìŒìš´ì „ ë“±)ë¥¼ ì¸ì‹ í•˜ê³ , ìœ„í—˜í•œ ì°¨ëŸ‰ì„ ì‚¬ì „ì— ì‹ë³„í•˜ì—¬ ì‚¬ê³ ë¥¼ ì˜ˆë°©í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„

ì°¨ëŸ‰ ì „ì¥ ì‹œìŠ¤í…œ(CANí†µì‹ ) ì—°ë™

## í”„ë¡œì íŠ¸ ëª©í‘œ

* ì£¼ë³€ ì°¨ëŸ‰ì˜ ìš´ì „ í–‰íƒœ ì¸ì‹ (ê³¼ì†, ì¡¸ìŒìš´ì „ ë“±)

* ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì²´ ì¶”ì  ë° ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

* CAN í†µì‹ ì„ í†µí•´ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ì—°ë™

* ì‹¤ì‹œê°„ ìœ„í—˜ ì°¨ëŸ‰ íƒì§€ ì‹œìŠ¤í…œ êµ¬ì¶•

## ì£¼ì œ ì„ ì • ì´ìœ 

* ì‚¬ê³ ë¥¼ ì‚¬ì „ì— ê°ì§€í•˜ê³  ì˜ˆë°©í•  ìˆ˜ ìˆëŠ” AI ì‹œìŠ¤í…œì˜ í•„ìš”ì„±

* í–¥í›„ ììœ¨ì£¼í–‰ ê¸°ëŠ¥ê³¼ì˜ ì—°ê³„ ê°€ëŠ¥ì„±, ë³´ë‹¤ ê²¬ê³ í•œ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ êµ¬ì¶•

* ì°¨ëŸ‰ ì „ì¥ ì‹œìŠ¤í…œì˜ í•µì‹¬ì¸ CAN í†µì‹ ì„ ì‹¤ë¬´ì ìœ¼ë¡œ ê²½í—˜í•˜ê³ ì í•¨

### Team: Overflowers

* Members
  | Name | Role |
  |----|----|
  |ì†¡ê°€ëŒ| íŒ€ì¥, í”„ë¡œì íŠ¸ ì´ê´„ ë° AI ë¹„ì „ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ |
  |í™©ì¹˜ì˜| CANí†µì‹  ë° í•˜ë“œì›¨ì–´ êµ¬í˜„ |
  |ì‹ ê²½ì„| CANí†µì‹  ë° í•˜ë“œì›¨ì–´ êµ¬í˜„ |

## High Level Design
### ê°œë°œí™˜ê²½
![ê°œë°œí™˜ê²½](https://github.com/user-attachments/assets/c4819215-83e8-4d50-b16c-ad70cfc3f378)

### ê°œë°œí™˜ê²½-í•˜ë“œì›¨ì–´
![ê°œë°œí™˜ê²½-í•˜ë“œì›¨ì–´](https://github.com/user-attachments/assets/d63a78cd-ccc1-41d5-bd3a-568e3696c252)
### ì‹œìŠ¤í…œ êµ¬ì„±ë„
![ì‹œìŠ¤í…œêµ¬ì„±ë„](https://github.com/user-attachments/assets/2e7f8dbf-c287-43cd-9bc6-8ee93fb2c671)

## Clone code

```shell
git clone https://github.com/GaramSong-95/Project-DrivingAI.git
```
## Prerequite

```shell
sudo apt update
sudo apt install can-utils
python m venv .venv
pip install boxmot
pip install python-can
```
If you want to run the RFDETR, YOLOX or YOLOv12 examples:
```shell
cd boxmot
pip install uv
uv sync --group yolo
activate .venv/bin/activate
```

## Steps to build

Project_STM í´ë”ë¥¼ STM32CUBEIDE í”„ë¡œê·¸ë¨ìœ¼ë¡œ import í›„ ë¹Œë“œ ì§„í–‰ ë³´ë“œëª…ì€ STM32F411RE

## Step to run

```shell
$ sudo ip link set can0 down  # í˜¹ì‹œ ì¼œì ¸ìˆë‹¤ë©´ ë¨¼ì € ë”
$ sudo ip link set can0 type can bitrate 500000
$ sudo ip link set can0 up
$ cd boxmot
$ python tracking/track.py --yolo-model yolov8n --source ì˜ìƒê²½ë¡œ --device 0 --veiw --save ì €ì¥ê²½ë¡œ
```

<details>
  <summary>Tracking</summary>
  
  ```shell
  
$ python tracking/track.py --yolo-model rf-detr-base.pt  # bboxes only
  python tracking/track.py --yolo-model yolox_s.pt       # bboxes only
  python tracking/track.py --yolo-model yolov10n         # bboxes only
  python tracking/track.py --yolo-model yolov9s          # bboxes only
  python tracking/track.py --yolo-model yolov8n          # bboxes only
                                        yolov8n-seg      # bboxes + segmentation masks 
                                        yolov8n-pose     # bboxes + pose estimation
```

</details>

<details>
  <summary>Tracking methods</summary>

  ```shell
$ python tracking/track.py --tracking-method deepocsort
                                             strongsort
                                             ocsort
                                             bytetrack
                                             botsort
                                             boosttrack
```

</details>

<details>
  <summary>Tracking sources</summary>
  
tracking can be run on most video formats
  ```shell
$ python tracking/track.py --source 0                               # webcam
                                    img.jpg                         # image
                                    vid.mp4                         # video
                                    path/                           # directory
                                    path/*.jpg                      # glob
                                    'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                    'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream
```

</details>

<details>
  <summary>select ReID model</summary>
  
Some tracking methods combine appearance description and motion in the process of tracking. For those which use appearance, you can choose a ReID model based on your needs from this ReID model zoo. These model can be further optimized for you needs by the reid_export.py script
  ```shell
$ python tracking/track.py --source 0 --reid-model lmbn_n_cuhk03_d.pt               # lightweight
                                                   osnet_x0_25_market1501.pt
                                                   mobilenetv2_x1_4_msmt17.engine
                                                   resnet50_msmt17.onnx
                                                   osnet_x1_0_msmt17.pt
                                                   clip_market1501.pt               # heavy
                                                   clip_vehicleid.pt
                                                   ...
```

</details>

<details>
  <summary>Filter tracked classes</summary>
  
By default the tracker tracks all MS COCO classes.
If you want to track a subset of the classes that you model predicts, add their corresponding index after the classes flag,
  ```shell
python tracking/track.py --source 0 --yolo-model yolov8s.pt --classes 16 17  # COCO yolov8 model. Track cats and dogs, only
```
Here is a list of all the possible objects that a Yolov8 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero

</details>

<details>
  <summary>Evaluation</summary>
  
Evaluate a combination of detector, tracking method and ReID model on standard MOT dataset or you custom one by
  ```shell
$ python3 tracking/val.py --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --tracking-method deepocsort --verbose --source ./assets/MOT17-mini/train
$ python3 tracking/val.py --yolo-model yolov8n.pt --reid-model osnet_x0_25_msmt17.pt --tracking-method ocsort     --verbose --source ./tracking/val_utils/MOT17/train
```
add --gsi to your command for postprocessing the MOT results by gaussian smoothed interpolation. Detections and embeddings are stored for the selected YOLO and ReID model respectively. They can then be loaded into any tracking algorithm. Avoiding the overhead of repeatedly generating this data.

</details>

<details>
  <summary>Evolution</summary>
  
We use a fast and elitist multiobjective genetic algorithm for tracker hyperparameter tuning. By default the objectives are: HOTA, MOTA, IDF1. Run it by
  ```shell
# saves dets and embs under ./runs/dets_n_embs separately for each selected yolo and reid model
$ python tracking/generate_dets_n_embs.py --source ./assets/MOT17-mini/train --yolo-model yolov8n.pt yolov8s.pt --reid-model weights/osnet_x0_25_msmt17.pt
# evolve parameters for specified tracking method using the selected detections and embeddings generated in the previous step
$ python tracking/evolve.py --dets yolov8n --embs osnet_x0_25_msmt17 --n-trials 9 --tracking-method botsort --source ./assets/MOT17-mini/train
```
The set of hyperparameters leading to the best HOTA result are written to the tracker's config file.

</details>

<details>
  <summary>Export</summary>
  
We support ReID model export to ONNX, OpenVINO, TorchScript and TensorRT
  ```shell
# export to ONNX
$ python3 boxmot/appearance/reid_export.py --include onnx --device cpu
# export to OpenVINO
$ python3 boxmot/appearance/reid_export.py --include openvino --device cpu
# export to TensorRT with dynamic input
$ python3 boxmot/appearance/reid_export.py --include engine --device 0 --dynamic
```

</details>

## Output

![ë³´ë“œì‚¬ì§„](https://github.com/user-attachments/assets/72f5e2cc-a8dc-40fc-88d8-2f6280a11a10)
![ì°¨ëŸ‰ì‚¬ì§„ jpg](https://github.com/user-attachments/assets/2781f0cc-47c7-48df-8aa3-a2aa0787fdd1)
![ì°¨ì‚¬ì§„](https://github.com/user-attachments/assets/9281e447-e93f-401b-b8ec-ef2724b98659)

## Appendix

* boxmot
https://github.com/mikel-brostrom/boxmot?tab=readme-ov-file#rfdetr--yolox--yolov12-examples
